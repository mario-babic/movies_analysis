{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ba5479-7659-42e6-9c68-e07509e147b1",
   "metadata": {},
   "source": [
    "# Porsche Assginment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed81212-2f2b-412c-b9d2-4f04110f946f",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e80aa4-273f-4613-aa3a-930d79509ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql.functions import collect_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1c62c-0fc7-4137-8ddd-7a55de4649f8",
   "metadata": {},
   "source": [
    "#### starting spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043ac4ab-1aaf-4da4-971d-b1459622f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Movies').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c5c3e-8598-48df-9aa7-154e7805a1d9",
   "metadata": {},
   "source": [
    "#### data location\n",
    "\n",
    "Current solution expect that data is exported in folder 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766625c1-230f-4dcf-957f-374d01fcac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data//'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fda0d-e092-45c2-bf3c-7936d7228ae0",
   "metadata": {},
   "source": [
    "#### load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c738dc6-abbf-4d72-ae06-688202ccec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(path_m, extension=\"csv\"):\n",
    "    file_names = []\n",
    "    for root, dirs, files in os.walk(path_m):\n",
    "        for name in files:\n",
    "            name_ext = name.split(\".\")[-1]\n",
    "            if name_ext == extension:\n",
    "                file_names.append(os.path.join(root, name))\n",
    "            \n",
    "    return file_names\n",
    "\n",
    "all_files = get_all_files(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d0066-fe4c-4214-ad55-08b8160bda71",
   "metadata": {},
   "source": [
    " #### loading data to dataframe\n",
    "Since data is not well formated, given that it has bad-json values with single quotes insead of double qoutes, and somewhere it has double quotes around these bad-jsons, and even somwhere for values inside bad-json it has a pair of double qoutes as a qoutation... and since as of yet, spark csv cannot be separated by regex - we do not read this file as csv but as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456dc441-fd3e-4d6f-8f41-6c64e2156768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files_into_df(all_files):\n",
    "    df = spark.read.text(all_files)\n",
    "    header=df.first()[0]\n",
    "    schema=header.split(\",\")\n",
    "\n",
    "    df = df.filter(~col(\"value\").contains(header))\n",
    "    \n",
    "    sep = r',(?! )'\n",
    "    new_sep = \";;;\"\n",
    "    df = df.withColumn('separated_values', regexp_replace(('value'), sep, new_sep))\n",
    "\n",
    "    for i in range(len(schema)):\n",
    "        df = df.withColumn(schema[i], split(col(\"separated_values\"), new_sep).getItem(i))\n",
    "    df = df[schema]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f70c840-5689-406c-901a-7f9e3739e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_for_json(dft, col='cast'):\n",
    "    dft2 = dft.withColumn(col, regexp_replace('cast', '\"\"', \"\"))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"\\\"\\[\\{\", '\\[\\{'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"\\}\\]\\\"\", '\\}\\]'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', '\"', \"\"))\n",
    "    \n",
    "    dft2 = dft.withColumn(col, regexp_replace('cast', \"\\[\\{'\", '\\[\\{\"'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"'\\}\\]\", '\"\\}\\]'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"\\{'\", '\\{\"'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"\\': \\'\", '\": \"'))\n",
    "    \n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"\\', \\'\", '\", \"'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \", \\'\", ', \"'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"\\': \", '\": '))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', \"'\\}\", '\"\\}'))\n",
    "\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', '\"\\[\\{', '\\[\\{'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', '\\}\\]\"', '\\}\\]'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', '\"\"', '`'))\n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', '`', '\"'))\n",
    "    \n",
    "    dft2 = dft2.withColumn(col, regexp_replace('cast', ': None', ': \"\"'))\n",
    "    \n",
    "    return dft2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8242ba-a824-4c3d-ad93-a6880cb43f5d",
   "metadata": {},
   "source": [
    "#### UDFs\n",
    "\n",
    "Given more time, reading these jsons, can be improved in a lots of ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068e3967-4210-4bb4-b2ce-d1f00ba117d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = ArrayType(StructType([\n",
    "    StructField(\"id\",StringType(),True), \n",
    "    StructField(\"name\",StringType(),True)\n",
    "  ]))\n",
    "\n",
    "json_schema_ids = ArrayType(\n",
    "   StringType(),True\n",
    "  )\n",
    "\n",
    "def parse_json(array_str):\n",
    "    try:\n",
    "        json_obj = json.loads(array_str)\n",
    "        for item in json_obj:\n",
    "            try:\n",
    "                yield (item[\"id\"].strip(), item[\"name\"].strip())\n",
    "            except:\n",
    "                yield (\"Null\", \"Null\")\n",
    "    except:\n",
    "        yield (\"Null\", \"Null\")\n",
    "        \n",
    "\n",
    "def fake_parse_jsons(array_str):\n",
    "    try:\n",
    "        k = array_str.split(\"}, {\")\n",
    "        for i in k:\n",
    "            t = i.split(\", \")\n",
    "            for p in t:\n",
    "                if '\"name\"' in p:\n",
    "                    name = p.split(\": \")[1]#[1,-1]\n",
    "                if '\"id\"' in p:\n",
    "                    ids = p.split(\": \")[1]\n",
    "            yield (ids.strip(), name.strip())\n",
    "    except:\n",
    "        yield None\n",
    "        #yield (\"Null\", \"Null\")\n",
    "        \n",
    "udf_parse_json = udf(lambda lst: parse_json(lst), json_schema)\n",
    "udf_parse_fake_json = udf(lambda lst: fake_parse_jsons(lst), json_schema) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89156699-fe10-43ae-a78d-1dc244fcf147",
   "metadata": {},
   "source": [
    "### Load data to dataframe, and preparing for task1 and task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b9f5a4-d664-4459-8957-684650cf1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_all_files_into_df(all_files)\n",
    "\n",
    "df = df.na.drop()\n",
    "\n",
    "df = prepare_df_for_json(df, 'cast')\n",
    "df = df.withColumn(\"cast_id_name\", udf_parse_fake_json(df.cast))\n",
    "\n",
    "df = prepare_df_for_json(df, 'crew')\n",
    "df = df.withColumn(\"crew_id_name\", udf_parse_fake_json(df.crew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ff55a5-0a2b-4e92-8ba6-846fb052c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task includes cast and crew, it can be added.\n",
    "\n",
    "df = df.withColumn(\"cast_id_name\", array_distinct(concat(col(\"cast_id_name\"), col(\"crew_id_name\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ec20b-c075-4f09-90f6-e37f14aa28db",
   "metadata": {},
   "source": [
    "## First task can be resolved as this dataframe. But we can also give a very large dict.\n",
    "Task 1 is to extract data about people from cast and crew on movie set. Data is in bad-json inside csvs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3b4b0f-153e-4c3f-937f-93f75763969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cast_full = df[[\"cast_id_name\"]]\n",
    "\n",
    "dft2_exploded = df_cast_full.withColumn('cast_id_name_single', explode('cast_id_name'))\n",
    "\n",
    "df_id_name = dft2_exploded.withColumn('id', col('cast_id_name_single')[\"id\"]).withColumn(\"name\", col('cast_id_name_single')[\"name\"])[['id',\"name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "624f6e0c-b5e5-4b15-8ed2-056ea508825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|    id|               name|\n",
      "+------+-------------------+\n",
      "| 67228| \"Kathryn Beaumont\"|\n",
      "| 67290|     \"Verna Felton\"|\n",
      "|  5833|          \"Ed Wynn\"|\n",
      "| 29283|    \"Richard Haydn\"|\n",
      "| 34759|\"Sterling Holloway\"|\n",
      "|142527|    \"Jerry Colonna\"|\n",
      "| 22602|  \"J. Pat O'Malley\"|\n",
      "| 67230|    \"Bill Thompson\"|\n",
      "| 93897|    \"Heather Angel\"|\n",
      "|132709|    \"Joseph Kearns\"|\n",
      "+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_id_name.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5dde8e-ac1a-47b1-9519-b6fe8baef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_id_name = df_id_name.dropDuplicates()\n",
    "#df_id_name.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01dea83-96e2-4d7c-9c4e-c26d0e7d45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently not working, maybe due to too much data, but true python dictionary for single node would be get in this way:\n",
    "\n",
    "\n",
    "# id_dict = df_id_name.toPandas().set_index('id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588a18f-365b-4415-890e-febbcbe63da4",
   "metadata": {},
   "source": [
    "## Second task is to get index with movie ids and actor ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9de2eff7-6fa7-4409-977f-9d20739b7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cast = df[[\"id\", \"cast_id_name\"]]\n",
    "df_cast = df_cast.withColumn(\"actor_ids\", df_cast.cast_id_name.id)\n",
    "#df_cast.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec9ca8e-a4a7-40df-8f6a-e1141a4b2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft2_exploded = df_cast.withColumn('actor_id', explode('actor_ids'))\n",
    "#dft2_exploded.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d0ba8bb-466c-4db8-b90e-13ecb9d7d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = dft2_exploded[[\"id\", \"actor_id\"]]\n",
    "#df_movies.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "569d6262-83c1-4167-8077-6400b8040347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_movies.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a48a54a-0f7c-411e-8da7-ab4ec42f7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_movies.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4fb3b0-4fa4-4cef-bdcb-a40bc5010209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|actor_id|                 ids|\n",
      "+--------+--------------------+\n",
      "|     100|[834, 18165, 4911...|\n",
      "|   10000|[855, 54845, 319,...|\n",
      "| 1000007|[96771, 162928, 1...|\n",
      "| 1000061|             [36799]|\n",
      "| 1000083|[17058, 596, 3769...|\n",
      "| 1000145|            [136336]|\n",
      "| 1000152|            [169726]|\n",
      "| 1000195|[104221, 108213, ...|\n",
      "| 1000197|             [17185]|\n",
      "|  100022|            [280617]|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouped_df = df_movies.groupby('actor_id').agg(collect_set('id').alias(\"ids\"))\n",
    "grouped_df = df_movies.groupby('actor_id').agg(collect_list('id').alias(\"ids\"))\n",
    "grouped_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c927a-132e-44d3-b0bd-8061ed745ae9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Things that should be fixed:\n",
    "\n",
    "    1. loading json data\n",
    "    \n",
    "    2. add option automatily download data given the link to data online\n",
    "    \n",
    "    3. task1: enable collecting data and export dictionary (not as dataframe, but as python dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "Solved:\n",
    "\n",
    "    4. task2: edit data, drop null data\n",
    "    \n",
    "    5. add crew data (currently there is some bug when concatenating cast data and crew data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d03bd23-481d-41bd-b1ec-2d74edb68c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version :3.3.0\n"
     ]
    }
   ],
   "source": [
    "print('PySpark Version :'+spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49536b2f-1d7d-409a-8f7e-e26f30f73cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
